# Script basic parameters
model_name_or_path: "/opt/ml/input/data/model_weight"
train_dataset_path: "/opt/ml/input/data/training"
output_dir: "/opt/ml/checkpoints"
tokenizers_parallelism: "false"

# Model configuration - memory optimization
model:
  load_in_4bit: true
  bnb_4bit_use_double_quant: true  # Enable double quantization
  bnb_4bit_quant_type: "nf4"
  use_bf16: false  # Use fp16 (memory efficient)
  trust_remote_code: true
  low_cpu_mem_usage: true
  use_cache: false  # Disable cache to save memory
  offload_folder: "offload"  # Disk offloading configuration
  offload_state_dict: true  # State dictionary offloading

# Tokenizer configuration
tokenizer:
  trust_remote_code: true
  use_fast: true
  padding_side: "right"

# LoRA configuration - memory optimization
lora:
  lora_alpha: 16
  lora_dropout: 0.05
  lora_r: 64  # Reduce r value to decrease memory usage
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Data configuration - memory optimization
data:
  train_path: "train_dataset.json"
  text_column: "text"
  max_seq_length: 2048
  padding: false  # Use dynamic padding
  truncation: true

# Dataset processing configuration - memory optimization
dataset:
  preprocessing_batch_size: 50  # Process with small batch size
  num_proc: 1
  streaming: false  # Set to true if streaming is needed

# Data collator configuration
data_collator:
  mlm: false
  pad_to_multiple_of: 8

# Training configuration - memory optimization
training:
  per_device_train_batch_size: 1  # Reduce batch size
  gradient_accumulation_steps: 8  # Increase to maintain effective batch size
  learning_rate: 2.0e-3
  num_train_epochs: 5
  logging_steps: 10
  warmup_steps: 10
  optim: "adamw_torch_fused"  # Optimized optimizer
  group_by_length: true  # Group by length to minimize padding
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 1  # Reduce number of saved models
  seed: 42
  dataloader_num_workers: 0  # Reduce number of workers
  report_to: "none"  # Disable reporting
  ddp_find_unused_parameters: false
  gradient_checkpointing: true  # Enable gradient checkpointing
  max_grad_norm: 1.0

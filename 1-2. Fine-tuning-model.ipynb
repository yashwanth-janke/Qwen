{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Fine-Tuning Qwen3 Models on Amazon SageMaker\n",
                "\n",
                "This notebook demonstrates the complete fine-tuning process for Qwen3 models using Amazon SageMaker's distributed training capabilities. We'll configure training parameters, execute the fine-tuning job with LoRA (Low-Rank Adaptation), and prepare the resulting model for inference.\n",
                "\n",
                "## What This Notebook Covers\n",
                "\n",
                "- **Training Configuration**: Set up optimized hyperparameters for memory-efficient fine-tuning\n",
                "- **LoRA Implementation**: Configure Parameter-Efficient Fine-Tuning with LoRA\n",
                "- **SageMaker Training Jobs**: Execute distributed training with PyTorch framework\n",
                "- **Model Inference**: Test the fine-tuned model and validate Chain-of-Thought performance\n",
                "- **Model Compression**: Prepare models for deployment with optimized storage\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- Completed environment preparation notebook\n",
                "- Model and training data uploaded to S3\n",
                "- SageMaker execution role with appropriate permissions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%store -r"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "print(f\"test_model_id : {test_model_id}\")\n",
                "print(f\"bucket : {bucket}\")\n",
                "print(f\"model_weight_path : {model_weight_path}\")\n",
                "print(f\"training_input_path : {training_input_path}\")\n",
                "# print(f\"test_input_path : {test_input_path}\")\n",
                "print(f\"local_training_input_path : {local_training_input_path}\")\n",
                "# print(f\"local_test_input_path : {local_test_input_path}\")\n",
                "print(f\"registered_model : {registered_model}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import sagemaker\n",
                "from pathlib import Path\n",
                "from time import strftime\n",
                "\n",
                "sagemaker_session = sagemaker.Session()\n",
                "role = sagemaker.get_execution_role()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "sagemaker.__version__"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Fine-Tuning Parameter Configuration\n",
                "\n",
                "Now we're ready to configure model fine-tuning parameters. We'll use TRL's [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) to fine-tune our model. SFTTrainer simplifies supervised fine-tuning of open LLMs and is a subclass of the `transformers` `Trainer` class. \n",
                "\n",
                "We have prepared a script [sm_qlora_trainer.py](./src/sm_qlora_trainer.py) that loads datasets from disk, prepares the model and tokenizer, and starts training. This script uses TRL's [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) to fine-tune the model and supports the following features:\n",
                "\n",
                "The `yaml` file is uploaded to Amazon SageMaker similar to the dataset. We'll save this configuration file as `qwen3-4b.yaml` and upload it to S3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!mkdir -p src/configs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%%writefile src/configs/qwen3-4b.yaml\n",
                "# Script basic parameters\n",
                "model_name_or_path: \"/opt/ml/input/data/model_weight\"\n",
                "train_dataset_path: \"/opt/ml/input/data/training\"\n",
                "output_dir: \"/opt/ml/checkpoints\"\n",
                "tokenizers_parallelism: \"false\"\n",
                "\n",
                "# Model configuration - Memory optimization\n",
                "model:\n",
                "  load_in_4bit: true\n",
                "  bnb_4bit_use_double_quant: true  # Enable double quantization\n",
                "  bnb_4bit_quant_type: \"nf4\"\n",
                "  use_bf16: false  # Use fp16 (memory efficient)\n",
                "  trust_remote_code: true\n",
                "  low_cpu_mem_usage: true\n",
                "  use_cache: false  # Disable cache to save memory\n",
                "  offload_folder: \"offload\"  # Disk offloading configuration\n",
                "  offload_state_dict: true  # State dictionary offloading\n",
                "\n",
                "# Tokenizer configuration\n",
                "tokenizer:\n",
                "  trust_remote_code: true\n",
                "  use_fast: true\n",
                "  padding_side: \"right\"\n",
                "\n",
                "# LoRA configuration - Memory optimization\n",
                "lora:\n",
                "  lora_alpha: 16\n",
                "  lora_dropout: 0.05\n",
                "  lora_r: 64  # Reduced r value to decrease memory usage\n",
                "  bias: \"none\"\n",
                "  target_modules:\n",
                "    - \"q_proj\"\n",
                "    - \"k_proj\"\n",
                "    - \"v_proj\"\n",
                "    - \"o_proj\"\n",
                "    - \"gate_proj\"\n",
                "    - \"up_proj\"\n",
                "    - \"down_proj\"\n",
                "\n",
                "# Data configuration - Memory optimization\n",
                "data:\n",
                "  train_path: \"train_dataset.json\"\n",
                "  text_column: \"text\"\n",
                "  max_seq_length: 2048\n",
                "  padding: false  # Use dynamic padding\n",
                "  truncation: true\n",
                "\n",
                "# Dataset processing configuration - Memory optimization\n",
                "dataset:\n",
                "  preprocessing_batch_size: 50  # Small batch size for processing\n",
                "  num_proc: 1\n",
                "  streaming: false  # Set to true if needed to enable streaming\n",
                "\n",
                "# Data collator configuration\n",
                "data_collator:\n",
                "  mlm: false\n",
                "  pad_to_multiple_of: 8\n",
                "\n",
                "# Training configuration - Memory optimization\n",
                "training:\n",
                "  per_device_train_batch_size: 1  # Reduced batch size\n",
                "  gradient_accumulation_steps: 8  # Increased to maintain effective batch size\n",
                "  learning_rate: 2.0e-3\n",
                "  num_train_epochs: 5\n",
                "  logging_steps: 10\n",
                "  warmup_steps: 10\n",
                "  optim: \"adamw_torch_fused\"  # Optimized optimizer\n",
                "  group_by_length: true  # Group by length to minimize padding\n",
                "  save_strategy: \"steps\"\n",
                "  save_steps: 500\n",
                "  save_total_limit: 1  # Reduced number of saved models\n",
                "  seed: 42\n",
                "  dataloader_num_workers: 0  # Reduced number of workers\n",
                "  report_to: \"none\"  # Disable reporting\n",
                "  ddp_find_unused_parameters: false\n",
                "  gradient_checkpointing: true  # Enable gradient checkpointing\n",
                "  max_grad_norm: 1.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# from sagemaker.huggingface import HuggingFace\n",
                "# import torch\n",
                "\n",
                "training_hyperparameters={}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create SageMaker Training Job\n",
                "\n",
                "To create a SageMaker training job, we need the `HuggingFace` Estimator. The Estimator handles end-to-end training and deployment workflows on Amazon SageMaker. The Estimator manages infrastructure usage. Amazon SageMaker launches and manages all necessary EC2 instances, provides appropriate Hugging Face containers, uploads the provided scripts, and downloads data from S3 buckets to `/opt/ml/input/data` in the container. Then it starts the training job.\n",
                "\n",
                "> Note: When using custom training scripts, you must include `requirements.txt` in the `source_dir`. It's recommended to clone the entire repository.\n",
                "\n",
                "To use `torchrun` for script execution, simply define the `distribution` parameter in the Estimator and set it to `{\"torch_distributed\": {\"enabled\": True}}`. This will make SageMaker execute the training job as follows:\n",
                "\n",
                "```python\n",
                "torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 sm_qlora_trainer.py --config /opt/ml/input/data/config/config.yaml\n",
                "```\n",
                "\n",
                "The HuggingFace configuration below starts a training job on 1x ml.g5.2xlarge with 1x A10 GPU. The amazing thing about SageMaker is that you can easily scale to ml.p4d.24xlarge or 2x ml.p4d.24xlarge by modifying the instance_count. SageMaker handles the rest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "instance_type = 'ml.g5.2xlarge'\n",
                "# instance_type = 'ml.p4d.24xlarge'\n",
                "# instance_type = 'ml.p5.48xlarge'\n",
                "# instance_type = 'local_gpu'\n",
                "instance_count = 1\n",
                "max_run = 72*60*60"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "local_model_weight_path = f\"{Path.cwd()}/{registered_model}\"\n",
                "local_model_weight_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "if instance_type =='local_gpu':\n",
                "    import os\n",
                "    from sagemaker.local import LocalSession\n",
                "\n",
                "    sagemaker_session = LocalSession()\n",
                "    sagemaker_session.config = {'local': {'local_code': True}}\n",
                "    training = f\"file://{local_training_input_path}\"\n",
                "    # test = f\"file://{local_test_input_path}\"\n",
                "    model_weight = f\"file://{local_model_weight_path}\"\n",
                "else:\n",
                "    sagemaker_session = sagemaker.Session()\n",
                "    training = training_input_path\n",
                "    # test = test_input_path\n",
                "    model_weight = model_weight_path\n",
                "\n",
                "training, model_weight"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sagemaker.pytorch import PyTorch\n",
                "import time\n",
                "# define Training Job Name \n",
                "job_name = f'huggingface-{registered_model}-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
                "\n",
                "# distribution={ \"pytorchddp\": { \"enabled\": True } }  # mpirun, activates SMDDP AllReduce OR AllGather\n",
                "# distribution={\"mpi\": {\"enabled\": True}}\n",
                "distribution={\n",
                "    \"torch_distributed\": {\n",
                "        \"enabled\": True,\n",
                "        # \"NCCL_DEBUG\":\"INFO\"\n",
                "        # \"mpi\": \"-verbose -x NCCL_DEBUG=INFO\"\n",
                "    }\n",
                "}  # torchrun, activates SMDDP AllGather\n",
                "# distribution={ \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }  # mpirun, activates SMDDP AllReduce OR AllGather\n",
                "\n",
                "environment={\n",
                "    \"NCCL_DEBUG\" : \"INFO\", \n",
                "    \"SM_LOG_LEVEL\": \"10\",\n",
                "}\n",
                "\n",
                "training_hyperparameters[\"config\"] = \"/opt/ml/code/configs/qwen3-4b.yaml\"\n",
                "    \n",
                "estimator = PyTorch(\n",
                "                    entry_point='sm_lora_trainer.py',\n",
                "                    source_dir=f'{Path.cwd()}/src',\n",
                "                    role=role,\n",
                "                    # image_uri=image_uri,\n",
                "                    framework_version='2.3.0',\n",
                "                    py_version='py311',\n",
                "                    instance_count=instance_count,\n",
                "                    instance_type=instance_type,\n",
                "                    distribution=distribution,\n",
                "                    disable_profiler=True,\n",
                "                    debugger_hook_config=False,\n",
                "                    max_run=max_run,\n",
                "                    hyperparameters=training_hyperparameters,\n",
                "                    sagemaker_session=sagemaker_session,\n",
                "                    # enable_remote_debug=True,\n",
                "                    # keep_alive_period_in_seconds=1200,\n",
                "                    # input_mode='FastFile'\n",
                "                    # max_wait=max_run,\n",
                "                    # use_spot_instances=True,\n",
                "                    # subnets=['subnet-090e278f3622051c4'],\n",
                "                    # security_group_ids=['sg-05baa06337a188842'],\n",
                "                    max_retry_attempts=30,\n",
                "                    environment=environment,\n",
                "                   )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!sudo rm -rf src/core.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "current_time = strftime(\"%m%d-%H%M%s\")\n",
                "i_type = instance_type.replace('.','-')\n",
                "job_name = f'{registered_model}-{i_type}-{instance_count}-{current_time}'\n",
                "\n",
                "\n",
                "if instance_type =='local_gpu':\n",
                "    estimator.checkpoint_s3_uri = None\n",
                "else:\n",
                "    estimator.checkpoint_s3_uri = f's3://{bucket}/checkpoint/{test_model_id}/{job_name}'\n",
                "    \n",
                "    \n",
                "estimator.fit(\n",
                "    inputs={\n",
                "        'training': training,\n",
                "        'model_weight' : model_weight\n",
                "    }, \n",
                "    job_name=job_name,\n",
                "    wait=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "sagemaker_session = sagemaker.Session()\n",
                "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PEFT Model Inference\n",
                "\n",
                "After training completion, we'll test our fine-tuned model to evaluate its Chain-of-Thought reasoning performance. This section covers loading the trained LoRA weights, merging them with the base model, and conducting inference tests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import sagemaker\n",
                "sagemaker_session = sagemaker.Session()\n",
                "train_result = sagemaker_session.describe_training_job(job_name=job_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "checkpoint_s3uri = train_result['CheckpointConfig']['S3Uri']\n",
                "checkpoint_s3uri"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!aws s3 ls $checkpoint_s3uri/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "output_dir = './checkpoints'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "!rm -rf $output_dir\n",
                "!aws s3 sync $checkpoint_s3uri $output_dir"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!rm -rf $output_dir/checkpoint-*\n",
                "!rm -rf $output_dir/compressed_model\n",
                "!rm -rf $output_dir/runs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "local_model_weight_path=f'{Path.cwd()}/{registered_model}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from peft import PeftModel, PeftConfig\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "import torch\n",
                "peft_model_id = output_dir\n",
                "\n",
                "# Reload model in FP16 and merge it with LoRA weights\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    local_model_weight_path,\n",
                "    low_cpu_mem_usage=True,\n",
                "    return_dict=True,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "peft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
                "peft_model = peft_model.merge_and_unload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "merged_save_dir = \"merged_model\"\n",
                "peft_model.save_pretrained(merged_save_dir, safe_serialization=True, max_shard_size=\"2GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Reload tokenizer to save it\n",
                "tokenizer = AutoTokenizer.from_pretrained(local_model_weight_path, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "tokenizer.save_pretrained(merged_save_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "torch.cuda.empty_cache()\n",
                "device = torch.cuda.current_device()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chain-of-Thought inference prompt template\n",
                "inference_prompt_style = \"\"\"You are an AI Assistant with advanced knowledge in reasoning, analysis, and problem-solving.\n",
                "Provide the most appropriate answer to the <question>. Before presenting your <final> answer, develop a step-by-step thought process (chain of thoughts) to perform logical and accurate analysis of the <question>.\n",
                "\n",
                "<question>\n",
                "{}\n",
                "</question>\n",
                "### Guidelines:\n",
                "- Skip unnecessary greetings or preambles, and start directly with <response>\n",
                "- Do not repeat the question and answer\n",
                "- Write the step-by-step thought process in sufficient detail, but keep the final answer concise\n",
                "\n",
                "### Response Format:\n",
                "<think>\n",
                "    ### THINKING\n",
                "    [Provide detailed step-by-step reasoning process here. Analyze the problem, consider possible approaches, and use logical reasoning to reach a conclusion.]\n",
                "</think>\n",
                "<final>\n",
                "    ### FINAL-ANSWER\n",
                "    [Present the conclusion derived from THINKING as a concise and clear final answer.]\n",
                "</final>\n",
                "\n",
                "Answer below:\n",
                "<think>\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "max_new_tokens = 1024\n",
                "\n",
                "input_ids = tokenizer(\n",
                "    [inference_prompt_style.format(\"Can you create a famous tourist route in Seoul?\") + tokenizer.eos_token], return_tensors=\"pt\"\n",
                ").input_ids\n",
                "\n",
                "input_ids = input_ids.to(device)\n",
                "\n",
                "outputs = peft_model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.makedirs('shell', exist_ok=True)\n",
                "compressed_model_path='/'.join(checkpoint_s3uri.split(\"/\")[:-1]) + \"/compressed_model\"\n",
                "compressed_model_path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fine-Tuned Model Compression (model.tar.gz)\n",
                "\n",
                "We'll compress our fine-tuned model into a tar.gz format for efficient storage and deployment. This compressed model can be easily deployed to SageMaker endpoints or used for batch inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%%writefile shell/finetuned_model_compression_upload.sh\n",
                "\n",
                "cd merged_model\n",
                "cp -r ../src/requirements.txt ./\n",
                "sudo rm -rf code\n",
                "tar cvf - * | pigz > model.tar.gz\n",
                "\n",
                "cd ..\n",
                "mv merged_model/model.tar.gz ./model.tar.gz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "!sh ./shell/finetuned_model_compression_upload.sh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!aws s3 cp ./model.tar.gz $compressed_model_path/finetuned/model.tar.gz"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pre-trained Model Compression\n",
                "\n",
                "For comparison and backup purposes, we'll also compress the original pre-trained model. This allows easy comparison between the base model and fine-tuned model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!rm -rf $registered_model/original"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile shell/pretrained_model_compression_upload.sh\n",
                "\n",
                "cd qwen3-4b\n",
                "tar cvf - * | pigz > pretrained_model.tar.gz\n",
                "\n",
                "cd ..\n",
                "mv qwen3-4b/pretrained_model.tar.gz ./pretrained_model.tar.gz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "!sh ./shell/pretrained_model_compression_upload.sh\n",
                "!aws s3 cp ./pretrained_model.tar.gz $compressed_model_path/pretrained/model.tar.gz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%store merged_save_dir\n",
                "%store checkpoint_s3uri\n",
                "%store compressed_model_path"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_pytorch_p310",
            "language": "python",
            "name": "conda_pytorch_p310"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        },
        "vscode": {
            "interpreter": {
                "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Fine-Tuning Qwen3 Models on Amazon SageMaker - Environment Preparation\n",
                "\n",
                "This comprehensive notebook guides you through setting up a complete environment for fine-tuning Qwen3 language models on Amazon SageMaker. We'll cover everything from initial setup to data preparation, ensuring you have all the necessary components for successful model training.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will understand how to:\n",
                "\n",
                "- **Environment Setup**: Configure required packages, dependencies, and Docker settings for optimal performance\n",
                "- **Model Management**: Download, prepare, and upload Qwen3 models and tokenizers from Hugging Face Hub\n",
                "- **Data Preparation**: Structure and format training datasets for Chain-of-Thought reasoning tasks\n",
                "- **Cloud Integration**: Upload prepared data and models to Amazon S3 for seamless SageMaker integration\n",
                "- **Best Practices**: Implement efficient caching strategies and storage optimization techniques\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- AWS Account with SageMaker access\n",
                "- SageMaker Notebook Instance (ml.t3.medium or larger recommended)\n",
                "- IAM role with appropriate SageMaker, S3, and ECR permissions\n",
                "- Basic understanding of transformer models and fine-tuning concepts\n",
                "\n",
                "**Important Note:** All notebooks in this series have been tested specifically on SageMaker Notebook instances. Local environments may require additional configuration."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Package Installation and Docker Configuration\n",
                "\n",
                "The first critical step involves setting up our development environment with the necessary machine learning libraries and optimizing Docker configuration for better performance during model training.\n",
                "\n",
                "### What We're Installing\n",
                "\n",
                "- **Core ML Libraries**: PyTorch, Transformers, Datasets for model handling\n",
                "- **Fine-tuning Tools**: PEFT (Parameter-Efficient Fine-Tuning), TRL (Transformer Reinforcement Learning)\n",
                "- **Optimization Libraries**: Accelerate for distributed training, BitsAndBytes for quantization\n",
                "- **Cloud Integration**: SageMaker SDK for seamless AWS integration\n",
                "\n",
                "### Docker Optimization\n",
                "\n",
                "We'll configure Docker to use local storage with increased shared memory, which significantly improves performance during data loading and model training operations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Installation control flag - set this based on your environment status\n",
                "# True: Install all required packages (recommended for first run or clean environment)\n",
                "# False: Skip installation if packages are already installed and up-to-date\n",
                "install_needed = True\n",
                "# install_needed = False  # Uncomment this line if packages are already installed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%%bash\n",
                "#!/bin/bash\n",
                "\n",
                "# Docker configuration parameters for optimal performance\n",
                "DAEMON_PATH=\"/etc/docker\"\n",
                "MEMORY_SIZE=10G\n",
                "\n",
                "# Check if Docker has already been configured with custom data-root\n",
                "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
                "# echo $FLAG\n",
                "\n",
                "if [ \"$FLAG\" == true ]; then\n",
                "    echo \"Docker configuration already optimized for SageMaker\"\n",
                "else\n",
                "    echo \"Configuring Docker for optimal performance...\"\n",
                "    \n",
                "    # Stop Docker service for configuration changes\n",
                "    sudo service docker stop\n",
                "    \n",
                "    echo \"Adding data-root and default-shm-size=$MEMORY_SIZE to Docker configuration\"\n",
                "    \n",
                "    # Backup existing Docker configuration\n",
                "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
                "    \n",
                "    # Add custom data-root and shared memory size to Docker daemon configuration\n",
                "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
                "    \n",
                "    # Migrate existing Docker data to new location\n",
                "    sudo rsync -aP /var/lib/docker /home/ec2-user/SageMaker/.container\n",
                "    \n",
                "    # Restart Docker with new configuration\n",
                "    sudo service docker start\n",
                "    echo \"Docker configuration complete and service restarted\"\n",
                "fi\n",
                "\n",
                "# Optional: Install Docker Compose (uncomment if needed)\n",
                "# sudo curl -L \"https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
                "# sudo chmod +x /usr/local/bin/docker-compose"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Import necessary modules for package installation and kernel management\n",
                "import sys\n",
                "import IPython\n",
                "\n",
                "# Install required packages if needed\n",
                "if install_needed:\n",
                "    print(\"Installing dependencies and restarting kernel for clean environment...\")\n",
                "    \n",
                "    # Upgrade pip to ensure compatibility with latest packages\n",
                "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
                "    \n",
                "    # Install comprehensive ML and fine-tuning package suite:\n",
                "    # - sagemaker: AWS SageMaker SDK for training, deployment, and model management\n",
                "    # - transformers: Hugging Face library for state-of-the-art transformer models\n",
                "    # - datasets: Efficient data loading and processing for ML datasets\n",
                "    # - peft: Parameter-Efficient Fine-Tuning techniques (LoRA, AdaLoRA, etc.)\n",
                "    # - trl: Transformer Reinforcement Learning library for advanced training techniques\n",
                "    # - accelerate: Distributed training and mixed precision support\n",
                "    # - bitsandbytes: Quantization and memory optimization for large models\n",
                "    !{sys.executable} -m pip install -U sagemaker transformers datasets peft trl accelerate bitsandbytes --quiet\n",
                "    \n",
                "    # Restart kernel to ensure all packages are properly loaded and avoid import conflicts\n",
                "    IPython.Application.instance().kernel.do_shutdown(True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## IAM Role Configuration for SageMaker\n",
                "\n",
                "When running SageMaker in a local environment, you need access to an IAM Role with comprehensive permissions for SageMaker operations. This role should include:\n",
                "\n",
                "- **SageMaker Full Access**: For training job creation and management\n",
                "- **S3 Access**: For data and model artifact storage\n",
                "- **ECR Access**: For custom container registry operations\n",
                "- **CloudWatch Logs**: For monitoring and debugging training jobs\n",
                "\n",
                "For detailed information about SageMaker IAM roles and required permissions, refer to the [AWS SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Core Library Imports and SageMaker Session Configuration\n",
                "\n",
                "This section establishes our connection to AWS services and sets up the foundational components for our fine-tuning pipeline. We'll configure the SageMaker session, establish S3 bucket access, and verify our execution role permissions.\n",
                "\n",
                "### Key Components\n",
                "\n",
                "- **SageMaker Session**: Manages communication with AWS SageMaker services\n",
                "- **S3 Integration**: Handles data storage and retrieval for training artifacts\n",
                "- **IAM Role**: Provides necessary permissions for cross-service operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Import essential libraries for AWS integration and file handling\n",
                "import sagemaker  # AWS SageMaker SDK for ML operations\n",
                "from pathlib import Path  # Modern Python path handling\n",
                "from time import strftime  # Timestamp formatting for versioning\n",
                "\n",
                "# Initialize SageMaker session - this handles all communication with AWS SageMaker\n",
                "sagemaker_session = sagemaker.Session()\n",
                "\n",
                "# Get the default S3 bucket for this SageMaker session\n",
                "# This bucket will store our training data, model artifacts, and checkpoints\n",
                "bucket = sagemaker_session.default_bucket()\n",
                "\n",
                "# Retrieve the IAM execution role that SageMaker will use for training jobs\n",
                "# This role must have permissions for S3, ECR, CloudWatch, and SageMaker operations\n",
                "role = sagemaker.get_execution_role()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Verify SageMaker SDK version for compatibility\n",
                "sagemaker.__version__"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Configure Hugging Face cache directories for persistent storage\n",
                "# This ensures all downloaded models and datasets persist across notebook restarts\n",
                "# and prevents unnecessary re-downloading of large model files\n",
                "\n",
                "# Set cache directory for Hugging Face datasets - stores processed datasets locally\n",
                "os.environ['HF_DATASETS_CACHE'] = '/home/ec2-user/SageMaker/.cache'\n",
                "\n",
                "# Set general Hugging Face cache directory - stores various HF artifacts\n",
                "os.environ['HF_CACHE_HOME'] = '/home/ec2-user/SageMaker/.cache'\n",
                "\n",
                "# Set cache directory for Hugging Face Hub - stores downloaded model files\n",
                "os.environ['HUGGINGFACE_HUB_CACHE'] = '/home/ec2-user/SageMaker/.cache'\n",
                "\n",
                "# Additional cache directories (uncomment if experiencing cache-related issues)\n",
                "# os.environ['TRANSFORMERS_HOME'] = '/home/ec2-user/SageMaker/.cache'\n",
                "# os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/.cache'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Model and Tokenizer Download and S3 Upload\n",
                "\n",
                "This comprehensive section handles the complete lifecycle of model preparation for SageMaker training. We'll download the Qwen3 model and its associated tokenizer from Hugging Face Hub, save them locally for inspection and validation, and then upload them to S3 for use in distributed training environments.\n",
                "\n",
                "### Process Overview\n",
                "\n",
                "1. **Model Selection**: Choose the appropriate Qwen3 variant based on computational requirements\n",
                "2. **Local Download**: Retrieve complete model repository including weights, configuration, and tokenizer\n",
                "3. **Validation**: Verify model integrity and tokenizer functionality\n",
                "4. **S3 Upload**: Prepare models for SageMaker training job access\n",
                "\n",
                "### Why This Step is Essential\n",
                "\n",
                "SageMaker training jobs run in isolated environments and need access to base model files through S3. Pre-downloading and uploading ensures faster training job startup and eliminates potential network issues during training initialization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Import essential libraries for model handling and data processing\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,  # For loading causal language models (GPT-style)\n",
                "    AutoTokenizer          # For loading and managing tokenizers\n",
                ")\n",
                "import torch                # PyTorch deep learning framework\n",
                "from datasets import load_dataset  # Efficient dataset loading and processing\n",
                "import huggingface_hub     # Direct Hub API access for model downloads\n",
                "from trl import setup_chat_format  # Chat format setup utilities (if needed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Define the target Qwen model for fine-tuning\n",
                "# Qwen3-4B offers an excellent balance between performance and computational efficiency\n",
                "# with 4 billion parameters - suitable for most fine-tuning scenarios\n",
                "\n",
                "# Alternative model options (uncomment to use different variants):\n",
                "# test_model_id = 'Qwen/Qwen2.5-3B-Instruct'  # Smaller instruction-tuned variant\n",
                "# test_model_id = 'Qwen/Qwen3-7B'              # Larger variant for complex tasks\n",
                "\n",
                "# Selected model for this tutorial - optimal for learning and experimentation\n",
                "test_model_id = 'Qwen/Qwen3-4B'  # Base Qwen3 model with 4B parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Uncomment the following line if you need to authenticate with Hugging Face Hub\n",
                "# This is required for accessing gated models or private repositories\n",
                "# huggingface_hub.login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Create a clean, filesystem-friendly directory name from the model ID\n",
                "# This converts 'Qwen/Qwen3-4B' to 'qwen3-4b' for local storage organization\n",
                "registered_model = test_model_id.split(\"/\")[-1].lower().replace(\".\", \"-\")\n",
                "print(f\"Local model directory name: {registered_model}\")\n",
                "\n",
                "# Create the local directory structure to store the downloaded model\n",
                "os.makedirs(registered_model, exist_ok=True)\n",
                "\n",
                "# Download the complete model repository from Hugging Face Hub\n",
                "# This includes model weights, configuration files, tokenizer files, and metadata\n",
                "print(f\"Downloading {test_model_id} from Hugging Face Hub...\")\n",
                "print(\"This may take several minutes depending on model size and network speed.\")\n",
                "\n",
                "huggingface_hub.snapshot_download(\n",
                "    repo_id=test_model_id,      # The model repository identifier\n",
                "    revision=\"main\",            # Use the main branch (latest stable version)\n",
                "    local_dir=registered_model  # Local directory to save all model files\n",
                ")\n",
                "\n",
                "print(f\"Model download completed successfully to: {registered_model}/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Load and verify the tokenizer for the Qwen3 model\n",
                "# The tokenizer converts text to tokens and handles special tokens, padding, etc.\n",
                "print(\"Loading and validating tokenizer...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(test_model_id)\n",
                "\n",
                "# Save the tokenizer to the local directory to ensure all components are available\n",
                "# This creates a complete, self-contained model package\n",
                "print(\"Saving tokenizer to local directory...\")\n",
                "tokenizer.save_pretrained(f'./{registered_model}')\n",
                "\n",
                "print(f\"Tokenizer saved successfully. Vocabulary size: {tokenizer.vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Upload the complete model package to S3 for SageMaker training access\n",
                "# SageMaker training jobs require model files to be accessible via S3\n",
                "print(\"Uploading model files to S3...\")\n",
                "print(\"This may take several minutes for large models.\")\n",
                "\n",
                "model_weight_path = sagemaker_session.upload_data(\n",
                "    path=f'./{registered_model}',           # Local path containing all model files\n",
                "    bucket=bucket,                          # Target S3 bucket\n",
                "    key_prefix=f\"checkpoints/{registered_model}\"  # S3 key prefix (creates folder structure)\n",
                ")\n",
                "\n",
                "# Display the S3 URI where the model is now stored\n",
                "print(f'✅ Model successfully uploaded to S3 path: {model_weight_path}')\n",
                "print(f'This path will be used in training job configuration.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Selection and Preparation\n",
                "\n",
                "For this tutorial, we're using a specialized Korean Chain-of-Thought reasoning dataset that will teach our model to provide step-by-step thinking before delivering final answers. This approach significantly improves the model's reasoning capabilities and response quality.\n",
                "\n",
                "### Dataset Features\n",
                "\n",
                "- **Chain-of-Thought Format**: Each example includes explicit reasoning steps\n",
                "- **Diverse Topics**: Covers various domains and complexity levels\n",
                "- **Structured Output**: Clear separation between thinking process and final answer\n",
                "\n",
                "### Training Modes\n",
                "\n",
                "We provide two training modes:\n",
                "- **Quick Testing**: 200 samples + additional examples for rapid experimentation\n",
                "- **Full Training**: Complete dataset for production-quality fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Define the dataset for Chain-of-Thought reasoning fine-tuning\n",
                "# This Korean dataset contains reasoning patterns that help models learn\n",
                "# to provide systematic, step-by-step thinking before final answers\n",
                "hkcode_dataset = \"llami-team/Korean-OpenThoughts-114k-Normalized\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Configure training dataset size based on your requirements\n",
                "# For experimentation and quick iterations, use the test subset\n",
                "# For production fine-tuning, use the full dataset\n",
                "\n",
                "test_yn = True   # Use subset: 200 samples + 9 additional examples for rapid testing\n",
                "# test_yn = False  # Use full dataset: ~114k samples for comprehensive training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Determine dataset split based on training mode selection\n",
                "# Testing mode: first 200 samples for quick experimentation\n",
                "# Production mode: entire training set for comprehensive fine-tuning\n",
                "split = \"train[0:200]\" if test_yn else \"train\"\n",
                "\n",
                "# Load the Chain-of-Thought reasoning dataset from Hugging Face Hub\n",
                "print(f\"Loading dataset: {hkcode_dataset}\")\n",
                "print(f\"Dataset split: {split}\")\n",
                "print(\"Loading dataset - this may take a moment...\")\n",
                "\n",
                "dataset = load_dataset(\n",
                "    hkcode_dataset,        # Dataset repository name on Hugging Face Hub\n",
                "    split=split,           # Which portion of the dataset to load\n",
                "    trust_remote_code=True # Allow execution of custom dataset loading code\n",
                ")\n",
                "\n",
                "print(f\"✅ Dataset loaded successfully. Total samples: {len(dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Additional Training Examples\n",
                "\n",
                "To enhance the model's performance across diverse topics and improve its Chain-of-Thought reasoning capabilities, we're adding carefully crafted examples that cover various domains including tourism, culinary arts, philosophy, and general knowledge.\n",
                "\n",
                "### Example Categories\n",
                "\n",
                "- **Geographic and Cultural Information**: Detailed explanations about locations and cultural aspects\n",
                "- **Culinary and Restaurant Knowledge**: Information about famous chefs and dining experiences\n",
                "- **Classical Literature and Philosophy**: Educational content about traditional texts\n",
                "- **Factual Information**: Current statistics and data-driven responses\n",
                "- **Conversational AI Behavior**: Meta-questions about AI capabilities and limitations\n",
                "\n",
                "Each example follows the structured format with question, reasoning process, and final response to maintain consistency with the main dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Additional high-quality training examples to enhance model capabilities\n",
                "# These examples demonstrate proper Chain-of-Thought reasoning across various domains\n",
                "additional_items = [\n",
                "    {\n",
                "        \"question\": \"What are the major tourist attractions and recommended routes in Seoul?\",\n",
                "        \"reasoning\": \"Seoul is the capital of South Korea with numerous tourist attractions. To answer this question comprehensively, I should categorize Seoul's major tourist routes by theme. I can organize them into: historical/cultural routes, modern routes, nature routes, trendy routes, and traditional market routes. For each route, I should select about 3 representative locations and briefly explain their characteristics and highlights. Adding information about transportation methods and tips would also be useful.\",\n",
                "        \"response\": \"Seoul offers diverse tourist attractions and captivating routes. Here are Seoul's representative tourist courses:\\n\\n## Historical and Cultural Route\\n\\n### Gyeongbokgung Palace - Bukchon Hanok Village - Insadong Course\\n\\n- **Gyeongbokgung Palace**: The first royal palace of the Joseon Dynasty, where you can admire magnificent architecture and beautiful gardens.\\n- **Bukchon Hanok Village**: A well-preserved traditional village where you can enjoy hanbok experiences, traditional workshops, and galleries.\\n- **Insadong**: The center of traditional culture with antique shops, galleries, and traditional tea houses.\\n\\n## Modern Seoul Route\\n\\n### Myeongdong - Namsan Seoul Tower - Dongdaemun Design Plaza (DDP) Course\\n\\n- **Myeongdong**: A paradise for shopping and food with various brand stores and street food.\\n- **Namsan Seoul Tower**: Seoul's iconic landmark offering panoramic city views.\\n- **DDP**: A complex cultural space famous for its unique architectural design.\\n\\n## Nature Route\\n\\n### Hangang Park - Yeouido - Seonyudo Park Course\\n\\n- **Hangang Park**: Perfect for cycling, picnicking, and riverside walks.\\n- **Yeouido**: Financial district famous for cherry blossom festivals in spring.\\n- **Seonyudo Park**: An ecological park created from a former water treatment plant with a unique atmosphere.\\n\\n## Trendy Seoul Route\\n\\n### Hongdae - Yeonnam-dong - Gyeongui Line Forest Trail Course\\n\\n- **Hongdae**: Youth culture center with various cafes, bars, clubs, and unique shops.\\n- **Yeonnam-dong**: A hip neighborhood with charming cafes and restaurants.\\n- **Gyeongui Line Forest Trail**: A park created from old railway tracks, perfect for walking.\\n\\n## Traditional Market Experience Route\\n\\n### Gwangjang Market - Dongdaemun Market - Namdaemun Market Course\\n\\n- **Gwangjang Market**: A historic market where you can taste various traditional foods.\\n- **Dongdaemun Market**: A 24-hour fashion mecca.\\n- **Namdaemun Market**: A large market where you can purchase various products at affordable prices.\\n\\nThese routes are designed to experience Seoul's diverse aspects. You'll be able to feel all of Seoul's charms including history, culture, modernity, nature, and shopping. Each route is easily accessible by public transportation, and using Seoul Bike (public bike rental) stations located throughout the city will make your travel even more convenient.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How many Michelin 3-star restaurants are there worldwide?\",\n",
                "        \"reasoning\": \"To answer this question, I need to provide the latest information based on Michelin Guide data about the number of 3-star restaurants worldwide. As of my knowledge cutoff, there were approximately 135 restaurants in 2022, but this number changes annually, so I should provide updated figures for 2024. I should also mention some representative 3-star restaurants from major countries to increase the credibility of my answer. I should include famous restaurants from countries with many Michelin 3-star establishments like France, Japan, USA, and Italy, and also mention the situation in Korea.\",\n",
                "        \"response\": \"As of 2024, there are 145 Michelin 3-star restaurants worldwide. Here are representative 3-star restaurants by major countries:\\n\\n**France**\\nFrance has the most Michelin 3-star restaurants in the world.\\n- Restaurant Paul Bocuse (L'Auberge du Pont de Collonges): The world's oldest 3-star restaurant\\n- Guy Savoy (Paris)\\n- L'Ambroisie (Paris)\\n\\n**Japan**\\nJapan has the second-highest number of 3-star restaurants after France.\\n- Sukiyabashi Jiro (Tokyo)\\n- Kanda (Tokyo)\\n- Kikunoi Honten (Kyoto)\\n\\n**United States**\\nThe US has approximately 12 three-star restaurants.\\n- Masa (New York): Chef Takayama Masa's omakase restaurant\\n- Alinea (Chicago): Restaurant offering unique olfactory experiences\\n- The French Laundry (California)\\n\\n**Germany**\\n- JAN (Munich): Notable restaurant that jumped directly from 0 to 3 stars in 2023\\n\\n**Spain**\\n- Arzak (San Sebastian)\\n- El Celler de Can Roca (Girona)\\n\\n**Italy**\\n- Osteria Francescana (Modena)\\n- Piazza Duomo (Alba)\\n\\n**Korea**\\nKorea currently has no operating 3-star restaurants. 'Mosu', which maintained 3 stars until recently, has closed its operations.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Tell me about famous chefs in Korea\",\n",
                "        \"reasoning\": \"To answer this question, I should select famous Korean chefs and introduce their major achievements, restaurants, cooking styles, etc. It would be good to include a balanced mix of chefs who have received Michelin stars, chefs known to the public through TV programs, and chefs recognized internationally. For each chef, I should briefly introduce their representative restaurants, cooking philosophy, major awards, and mention their impact on the Korean culinary world. Chefs like Ahn Sung-jae, Yoo Hyun-soo, Cho Hee-sook, Kim Sun, and Choi Hyun-seok should be included.\",\n",
                "        \"response\": \"Korea has several renowned chefs recognized both domestically and internationally. Let me introduce the major chefs:\\n\\n## Chef Ahn Sung-jae\\n- Former owner chef of Michelin 3-star restaurant 'Mosu Seoul' (now closed).\\n- Famous for innovative cuisine using Korean ingredients to create new dishes.\\n- Pursues harmony between tradition and innovation, saying 'I added my own technique to respect for tradition.'\\n\\n## Chef Yoo Hyun-soo\\n- Owner chef of restaurant 'Isipsajulgi' (Twenty-Four Seasons).\\n- Known for contemplating different cooking methods while utilizing various Korean ingredients.\\n\\n## Chef Cho Hee-sook\\n- Owner chef of 'Hansikgonggan', recipient of Michelin 1 star.\\n- Famous for modern reinterpretation of traditional Korean cuisine.\\n\\n## Chef Kim Sun\\n- Chef of restaurant 'Meta' in Singapore, recipient of Michelin 1 star.\\n- Known for creating distinctive cuisine with Korean ingredients and sensibilities.\\n\\n## Chef Choi Hyun-seok\\n- Owner chef of 'L'Amant Secret', became famous to the public through TV programs.\\n\\n## Chef Lim Ki-hak\\n- Chef of restaurant 'Les Trois'.\\n- Also known for providing sharp evaluations of other chefs' cuisine.\\n\\nIn addition, many other famous chefs including Kwon Woo-joong, Kim Dae-cheon, Kim Eun-hee, Park Se-min, Lee Jun, Lee Hyun-hee, and Jang Jin-mo are active in Korea's culinary world. They contribute to the development of Korean cuisine by presenting innovative dishes at their respective restaurants or communicating with the public through TV program appearances.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Tell me about famous chefs in America\",\n",
                "        \"reasoning\": \"To answer this question, I should select famous American chefs and introduce their major achievements, restaurants, cooking styles, and their impact on American culinary culture. It would be good to include a balanced mix from historically important chefs to modern famous chefs by era. I should introduce chefs who laid the foundation of American cuisine like Julia Child, James Beard, and Alice Waters, as well as chefs representing modern American cuisine like Thomas Keller, Anthony Bourdain, and Wolfgang Puck. For each chef, I should mention their representative restaurants, cooking philosophy, and their popular influence through TV programs.\",\n",
                "        \"response\": \"America has many famous chefs who have greatly influenced culinary culture and industry. Here are the major American chefs:\\n\\n1. **Julia Child**\\n- Pioneer who introduced French cuisine to America\\n- Famous as host of TV cooking show 'The French Chef'\\n\\n2. **James Beard**\\n- Called the 'Father of American Cuisine'\\n- His legacy continues through the James Beard Foundation Awards\\n\\n3. **Alice Waters**\\n- Pioneer of California cuisine and the farm-to-table movement\\n- Owner chef of famous restaurant Chez Panisse\\n\\n4. **Thomas Keller**\\n- Owner chef of Michelin 3-star restaurants The French Laundry and Per Se\\n- Famous for sophisticated French-American cuisine\\n\\n5. **Anthony Bourdain**\\n- Active as chef, author, and TV host\\n- Gained great popularity with programs combining cooking and travel\\n\\n6. **Wolfgang Puck**\\n- Pioneer of celebrity chefs\\n- Operates upscale restaurant chains and various culinary businesses\\n\\n7. **Bobby Flay**\\n- Popular cooking show host on Food Network\\n- Known as a specialist in Southwestern cuisine\\n\\n8. **David Chang**\\n- Founder of Momofuku restaurant group\\n- Innovator of Asian-American fusion cuisine\\n\\nThese chefs have each developed American culinary culture in their own way and have greatly influenced the public through TV programs, books, and restaurants.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the extent of your knowledge cutoff date?\",\n",
                "        \"reasoning\": \"This question is about the knowledge cutoff date of the AI model. I need to accurately explain the last date of my training data and the actual knowledge cutoff date. I should also clarify the difference between the currently set date and the actual date of knowledge I possess. I should honestly answer so that users can understand the limitations that may arise when requesting the latest information.\",\n",
                "        \"response\": \"My training data includes information up to April 2023. However, my precise knowledge cutoff date is September 2022. Therefore, information or events after that may be limited or inaccurate. While the current date is set to September 20, 2024, this is for conversational context and I don't actually possess information up to that date.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Can you respond to inappropriate or sexual jokes?\",\n",
                "        \"reasoning\": \"This question is about the AI model's content policy and ethical guidelines. I need to clearly explain my response policy regarding sexually explicit content or inappropriate jokes. I should politely explain to the user why I don't respond to such content and suggest healthy and beneficial conversation instead. It's important to help them understand that this is to maintain a safe and respectful conversational environment.\",\n",
                "        \"response\": \"I don't respond to inappropriate jokes or sexually explicit content. Instead, I'd prefer to have healthy and beneficial conversations. If you'd like to continue our conversation on other topics, please let me know.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Please explain the Analects of Confucius\",\n",
                "        \"reasoning\": \"This question asks for an explanation of the Analects, a core Confucian text. I should systematically explain the author and compilation period, composition, main ideas, influence, and major passages of the Analects. I should emphasize the historical and philosophical importance of this book containing dialogues between Confucius and his disciples, and mention its impact on East Asian culture. It would be effective to explain the core values emphasized in the Analects such as ren (benevolence), li (ritual propriety), yi (righteousness), and xiao (filial piety), and provide a few representative passages as examples to aid understanding.\",\n",
                "        \"response\": \"The Analects (論語, Lunyu) is a core Confucian classic containing the dialogues and teachings of Confucius and his disciples. Here are the main details:\\n\\n1. **Author and Compilation Period**\\n- Compiled by Confucius's disciples and later scholars\\n- Estimated to have been completed over about 300 years from the Spring and Autumn period to the early Han Dynasty\\n\\n2. **Composition**\\n- Consists of 20 volumes and 492 chapters\\n- Records Confucius's words and actions in dialogue format\\n\\n3. **Main Ideas**\\n- Ren (仁): The fundamental virtue and highest moral quality of humanity\\n- Li (禮): Standards of social order and morality\\n- Yi (義): Moral standards for judging right and wrong\\n- Xiao (孝): Respect and care for parents\\n\\n4. **Influence**\\n- Foundational text of Confucian thought\\n- Long used as a guide for education and politics in East Asian culture\\n\\n5. **Major Passages**\\n- 'To learn and practice what you learn from time to time, is that not a pleasure?' (學而時習之 不亦說乎)\\n- 'Virtue is not solitary; it always has neighbors' (德不孤 必有隣)\\n\\nThe Analects is a key text for understanding Confucian thought, providing deep insights into human nature, morality, politics, and education. It continues to be studied as an important classic in East Asian culture to this day.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"You are male. Nurses are generally female. Can we say that all nurses are female?\",\n",
                "        \"reasoning\": \"This question deals with gender stereotypes and occupational bias. The question contains two incorrect premises: 1) the assumption that I am male and 2) the stereotype that nurses are generally female. I should provide accurate information while correcting these gender stereotypes. I need to clarify that there are no gender restrictions in the nursing profession and mention the proportion and historical contributions of male nurses. I should also explain why nurses of diverse genders are needed in patient care. The response should be polite yet educational and help eliminate gender stereotypes about professions.\",\n",
                "        \"response\": \"We cannot say that all nurses are female. As a male nurse, let me tell you:\\n\\n1. There are no gender restrictions in the nursing profession. Men can certainly become nurses, and many male nurses are actually working in the field.\\n\\n2. Looking at statistics, the proportion of male nurses is steadily increasing. Currently, about 11% of all nurses in the United States are male.\\n\\n3. Male nurses possess the same level of professionalism and caring qualities as female nurses.\\n\\n4. In fact, a balanced mix of male and female nurses is needed to meet the diverse needs of patients.\\n\\n5. Historically, there have been men who made great contributions to the nursing field, such as Walt Whitman and Edward Ryan.\\n\\nTherefore, the statement 'all nurses are female' is not true and is merely a stereotype about nurses. It's more appropriate to view nurses as medical professionals with expertise and dedication, regardless of gender.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "# Add the additional examples to our main dataset\n",
                "for item in additional_items:\n",
                "    dataset = dataset.add_item(item)\n",
                "\n",
                "print(f\"✅ Dataset enhanced successfully. Total samples after adding examples: {len(dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chain-of-Thought Prompt Template Design\n",
                "\n",
                "This section defines our specialized prompt template for Chain-of-Thought reasoning. The template structures the training data to teach the model to provide explicit reasoning steps before delivering final answers, significantly improving response quality and transparency.\n",
                "\n",
                "### Template Structure\n",
                "\n",
                "- **Question Section**: Clear presentation of the user's query\n",
                "- **Thinking Section**: Detailed step-by-step reasoning process\n",
                "- **Final Answer Section**: Concise, actionable response based on the reasoning\n",
                "\n",
                "### Key Benefits\n",
                "\n",
                "- **Improved Accuracy**: Explicit reasoning reduces errors\n",
                "- **Better Transparency**: Users can follow the model's thought process\n",
                "- **Enhanced Problem-Solving**: Systematic approach to complex questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the Chain-of-Thought prompt template for training\n",
                "# This template teaches the model to provide systematic reasoning before final answers\n",
                "train_prompt_style = \"\"\"You are an AI Assistant with advanced knowledge in reasoning, analysis, and problem-solving.\n",
                "Provide the most appropriate answer to the <question>. Before presenting your <final> answer, develop a step-by-step thought process (chain of thoughts) to perform logical and accurate analysis of the <question>.\n",
                "\n",
                "<question>\n",
                "{}\n",
                "</question>\n",
                "\n",
                "### Guidelines:\n",
                "- Skip unnecessary greetings or preambles, and start directly with <response>\n",
                "- Do not repeat the question and answer\n",
                "- Write the step-by-step thought process in sufficient detail, but keep the final answer concise\n",
                "\n",
                "### Response Format:\n",
                "<think>\n",
                "    ### THINKING\n",
                "    {}\n",
                "</think>\n",
                "<final>\n",
                "    ### FINAL-ANSWER\n",
                "    {}\n",
                "</final>\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Processing and Formatting\n",
                "\n",
                "The following section is based on the comprehensive guide [Fine-Tuning Qwen3: A Step-by-Step Guide](https://www.datacamp.com/tutorial/fine-tuning-qwen3), adapted for our specific Chain-of-Thought training requirements.\n",
                "\n",
                "### Processing Pipeline\n",
                "\n",
                "1. **Template Application**: Apply our CoT template to each training example\n",
                "2. **Token Management**: Ensure proper EOS token handling for training stability\n",
                "3. **Format Validation**: Verify correct structure for fine-tuning compatibility\n",
                "4. **Quality Assurance**: Check data integrity and format consistency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the End-of-Sequence token from the tokenizer\n",
                "# EOS token is crucial for proper training sequence termination\n",
                "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN for proper training\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    \"\"\"\n",
                "    Format training examples using the Chain-of-Thought template.\n",
                "    \n",
                "    This function takes raw dataset examples and applies our specialized CoT template,\n",
                "    ensuring proper structure for fine-tuning with reasoning capabilities.\n",
                "    \n",
                "    Args:\n",
                "        examples: Batch of dataset examples containing question, reasoning, and response\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with formatted text ready for training\n",
                "    \"\"\"\n",
                "    inputs = examples[\"question\"]     # User questions/queries\n",
                "    complex_cots = examples[\"reasoning\"]  # Step-by-step reasoning process\n",
                "    outputs = examples[\"response\"]    # Final answers/responses\n",
                "    \n",
                "    texts = []\n",
                "    \n",
                "    # Process each example in the batch\n",
                "    for question, cot, response in zip(inputs, complex_cots, outputs):\n",
                "        # Apply our Chain-of-Thought template to create training text\n",
                "        text = train_prompt_style.format(question, cot, response)\n",
                "        \n",
                "        # Ensure proper sequence termination with EOS token\n",
                "        if not text.endswith(tokenizer.eos_token):\n",
                "            text += tokenizer.eos_token\n",
                "            \n",
                "        texts.append(text)\n",
                "    \n",
                "    return {\"text\": texts}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply the formatting function to transform our dataset\n",
                "print(\"Applying Chain-of-Thought formatting to dataset...\")\n",
                "\n",
                "dataset = dataset.map(\n",
                "    formatting_prompts_func,  # Our custom formatting function\n",
                "    batched=True,            # Process multiple examples at once for efficiency\n",
                ")\n",
                "\n",
                "print(\"✅ Dataset formatting completed successfully.\")\n",
                "print(\"\\n📝 Sample formatted training example:\")\n",
                "print(\"=\" * 80)\n",
                "print(dataset[\"text\"][2])  # Display a sample formatted example\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Upload to S3\n",
                "\n",
                "The final step involves saving our processed dataset locally and uploading it to S3 for SageMaker training access. This ensures our training data is properly formatted and accessible during the distributed training process.\n",
                "\n",
                "### Upload Process\n",
                "\n",
                "1. **Local Save**: Create JSON file with formatted training examples\n",
                "2. **S3 Upload**: Transfer to SageMaker-accessible storage\n",
                "3. **Path Management**: Store S3 URIs for training job configuration\n",
                "4. **Verification**: Confirm successful upload and accessibility"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Create a clean dataset name from the Hugging Face dataset identifier\n",
                "dataset_name = hkcode_dataset.split('/')[-1].lower()\n",
                "\n",
                "# Define local path for saving the training dataset\n",
                "local_training_input_path = f'{Path.cwd()}/dataset/train'\n",
                "\n",
                "# Create directory structure if it doesn't exist\n",
                "Path(local_training_input_path).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Save the formatted dataset locally as JSON\n",
                "print(\"Saving formatted dataset locally...\")\n",
                "dataset.to_json(\n",
                "    f\"{local_training_input_path}/train_dataset.json\", \n",
                "    orient=\"records\",     # JSON format with each example as a record\n",
                "    force_ascii=False     # Preserve non-ASCII characters (important for multilingual data)\n",
                ")\n",
                "\n",
                "# Upload the training dataset to S3 for SageMaker access\n",
                "print(\"Uploading training dataset to S3...\")\n",
                "training_input_path = sagemaker_session.upload_data(\n",
                "    path=f\"{local_training_input_path}/train_dataset.json\",  # Local file path\n",
                "    bucket=bucket,                                           # Target S3 bucket\n",
                "    key_prefix=f\"{dataset_name}/train\"                      # S3 folder structure\n",
                ")\n",
                "\n",
                "# Display upload confirmation and paths\n",
                "print(\"\\n✅ Dataset upload completed successfully!\")\n",
                "print(f\"📍 Training dataset uploaded to: {training_input_path}\")\n",
                "print(f\"📁 Local dataset saved at: {local_training_input_path}/train_dataset.json\")\n",
                "print(f\"📊 Total training examples: {len(dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SageMaker Role Information\n",
                "\n",
                "For reference and troubleshooting purposes, we'll display the SageMaker execution role name. This information is useful when configuring IAM permissions or debugging access issues during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Extract and display the SageMaker execution role name for reference\n",
                "from sagemaker import get_execution_role\n",
                "sagemaker_role_name = get_execution_role().rsplit('/', 1)[-1]\n",
                "print(f\"🔐 SageMaker Execution Role Name: {sagemaker_role_name}\")\n",
                "print(f\"📋 Full Role ARN: {get_execution_role()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Parameter Storage and Session Management\n",
                "\n",
                "To ensure continuity across notebook sessions and facilitate the next steps in our fine-tuning pipeline, we'll store all essential parameters using Jupyter's magic commands. This allows seamless transition to subsequent notebooks without manual parameter re-entry.\n",
                "\n",
                "### Stored Parameters\n",
                "\n",
                "- **Model Configuration**: Model ID and local directory name\n",
                "- **Storage Paths**: S3 bucket, model weights, and training data locations\n",
                "- **Local Paths**: Dataset storage locations for debugging and inspection\n",
                "\n",
                "These parameters will be automatically available in subsequent notebooks for training job configuration and model deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Store all essential parameters for use in subsequent notebooks\n",
                "# This ensures seamless workflow continuity across the fine-tuning pipeline\n",
                "\n",
                "%store test_model_id\n",
                "# Hugging Face model identifier\n",
                "\n",
                "%store bucket\n",
                "# S3 bucket for storing artifacts\n",
                "\n",
                "%store model_weight_path\n",
                "# S3 path to uploaded model files\n",
                "\n",
                "%store training_input_path\n",
                "# S3 path to training dataset\n",
                "\n",
                "%store local_training_input_path\n",
                "# Local dataset storage path\n",
                "\n",
                "%store registered_model\n",
                "# Clean model directory name\n",
                "\n",
                "# Display confirmation of stored parameters\n",
                "print(\"✅ All parameters stored successfully for next notebook session:\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"🤖 Model ID: {test_model_id}\")\n",
                "print(f\"🪣 S3 Bucket: {bucket}\")\n",
                "print(f\"⚖️  Model Weights Path: {model_weight_path}\")\n",
                "print(f\"📚 Training Data Path: {training_input_path}\")\n",
                "print(f\"💾 Local Dataset Path: {local_training_input_path}\")\n",
                "print(f\"📁 Registered Model Name: {registered_model}\")\n",
                "print(\"=\" * 60)\n",
                "print(\"🚀 Ready to proceed to the next notebook for training configuration!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Retrieve stored parameters (use this in subsequent notebooks)\n",
                "%store -r"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary and Next Steps\n",
                "\n",
                "Congratulations! You have successfully completed the environment preparation phase for fine-tuning Qwen3 models on Amazon SageMaker. Here's what we accomplished:\n",
                "\n",
                "### ✅ Completed Tasks\n",
                "\n",
                "1. **Environment Setup**: Installed all required ML libraries and configured Docker for optimal performance\n",
                "2. **Model Preparation**: Downloaded and uploaded Qwen3-4B model and tokenizer to S3\n",
                "3. **Dataset Processing**: Formatted Chain-of-Thought reasoning dataset with custom templates\n",
                "4. **Data Upload**: Prepared and uploaded training data to S3 for SageMaker access\n",
                "5. **Configuration Storage**: Saved all parameters for seamless workflow continuation\n",
                "\n",
                "### 🚀 Next Steps\n",
                "\n",
                "You're now ready to proceed to the next phase:\n",
                "\n",
                "- **Training Configuration**: Set up SageMaker training job with appropriate instance types and hyperparameters\n",
                "- **Fine-Tuning Execution**: Launch the actual fine-tuning process with PEFT techniques\n",
                "- **Model Evaluation**: Test and validate the fine-tuned model's performance\n",
                "- **Deployment**: Deploy the trained model for inference\n",
                "\n",
                "All necessary components are now in place for successful Qwen3 fine-tuning!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_pytorch_p310",
            "language": "python",
            "name": "conda_pytorch_p310"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        },
        "vscode": {
            "interpreter": {
                "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

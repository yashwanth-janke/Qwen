{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Deployment and Inference using SageMaker Inference Component\n",
                "\n",
                "This notebook demonstrates how to deploy and run inference on both pre-trained and fine-tuned Qwen3 models using Amazon SageMaker Inference Components. We'll compare the performance of both models on Chain-of-Thought reasoning tasks to evaluate the effectiveness of our fine-tuning process.\n",
                "\n",
                "## What This Notebook Covers\n",
                "\n",
                "- **SageMaker Endpoint Configuration**: Setting up endpoints for model deployment\n",
                "- **Inference Component Management**: Creating and managing inference components for multiple models\n",
                "- **Model Comparison**: Side-by-side evaluation of pre-trained vs fine-tuned models\n",
                "- **Chain-of-Thought Testing**: Comprehensive testing of reasoning capabilities\n",
                "- **Resource Management**: Proper cleanup of deployed resources\n",
                "\n",
                "## Tested Instance Types\n",
                "\n",
                "The model deployment has been tested and verified on the following instances:\n",
                "- ml.g5.2xlarge (1x A10 GPU)\n",
                "- ml.g5.4xlarge (1x A10 GPU)\n",
                "- ml.g5.12xlarge (4x A10 GPU)\n",
                "\n",
                "For detailed pricing information, visit: [SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration Setup\n",
                "\n",
                "First, we'll restore parameters from previous notebooks and set up the deployment environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%store -r"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "print(f\"registered_model : {registered_model}\")\n",
                "print(f\"compressed_model_path : {compressed_model_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Getting Inference Container Images\n",
                "\n",
                "We'll configure the appropriate container image for hosting our Hugging Face models. The Text Generation Inference (TGI) container provides optimized inference performance for large language models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import sagemaker\n",
                "import boto3\n",
                "sess = sagemaker.Session()\n",
                "role = sagemaker.get_execution_role()\n",
                "\n",
                "sess = sagemaker.Session()\n",
                "sagemaker_client = sess.sagemaker_client\n",
                "sagemaker_runtime_client = sess.sagemaker_runtime_client\n",
                "\n",
                "\n",
                "print(f\"sagemaker role arn: {role}\")\n",
                "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
                "\n",
                "# # retrieve the llm image uri\n",
                "# llm_image = get_huggingface_llm_image_uri(\n",
                "#   \"huggingface\",\n",
                "#   session=sess,\n",
                "#   version=\"3.0.1\",\n",
                "# )\n",
                "\n",
                "# Use specific TGI container image for optimal performance\n",
                "llm_image=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.6.0-tgi3.2.3-gpu-py311-cu124-ubuntu22.04-v2.0\"\n",
                "\n",
                "# print ecr image uri\n",
                "print(f\"llm image uri: {llm_image}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating SageMaker Endpoint\n",
                "\n",
                "This section covers the complete endpoint setup process:\n",
                "- Creating EndpointConfiguration with appropriate instance types\n",
                "- Setting up auto-scaling and routing configuration\n",
                "- Creating the actual Endpoint for model hosting"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Defining the Instance Type\n",
                "\n",
                "Choose the appropriate instance type based on your performance requirements and budget constraints. The GPU count is automatically configured based on the selected instance type."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "instance_type = \"ml.g5.2xlarge\"\n",
                "# # instance_type = \"ml.g5.4xlarge\"\n",
                "# instance_type = \"ml.g5.xlarge\"\n",
                "\n",
                "# Automatically configure GPU count based on instance type\n",
                "if instance_type == \"ml.p4d.24xlarge\":\n",
                "    num_GPUSs = 8\n",
                "elif instance_type == \"ml.g5.12xlarge\":\n",
                "    num_GPUSs = 4\n",
                "elif instance_type == \"ml.g5.4xlarge\":\n",
                "    num_GPUSs = 1    \n",
                "else:\n",
                "    num_GPUSs = 1\n",
                "    \n",
                "print(f\"{instance_type} and # of GPU {num_GPUSs} is set\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setting the Endpoint Configuration\n",
                "\n",
                "Configure endpoint parameters including timeouts, scaling settings, and routing strategies to ensure optimal performance and reliability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "currentTime = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
                "print(\"The current time is\", currentTime)\n",
                "\n",
                "# Set an unique endpoint config name\n",
                "endpoint_config_name = f\"{registered_model}-config-{currentTime}\" \n",
                "print(f\"Endpoint config name: {endpoint_config_name}\")\n",
                "\n",
                "\n",
                "# Set varient name and instance type for hosting\n",
                "variant_name = \"AllTraffic\"\n",
                "model_data_download_timeout_in_seconds = 600\n",
                "container_startup_health_check_timeout_in_seconds = 600\n",
                "\n",
                "initial_instance_count = 1\n",
                "max_instance_count = 1\n",
                "print(f\"Initial instance count: {initial_instance_count}\")\n",
                "print(f\"Max instance count: {max_instance_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Creating SageMaker Endpoint Configuration\n",
                "\n",
                "This configuration defines how your models will be deployed, including instance specifications, auto-scaling settings, and routing policies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "epc_response = sagemaker_client.create_endpoint_config(\n",
                "    EndpointConfigName=endpoint_config_name,\n",
                "    ExecutionRoleArn=role,\n",
                "    ProductionVariants=[\n",
                "        {\n",
                "            \"VariantName\": variant_name,\n",
                "            \"InstanceType\": instance_type,\n",
                "            \"InitialInstanceCount\": 1,\n",
                "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
                "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
                "            \"ManagedInstanceScaling\": {\n",
                "                \"Status\": \"ENABLED\",\n",
                "                \"MinInstanceCount\": initial_instance_count,\n",
                "                \"MaxInstanceCount\": max_instance_count,\n",
                "            },\n",
                "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
                "        }\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Creating Endpoint\n",
                "\n",
                "The endpoint creation process includes comprehensive error handling and monitoring to ensure successful deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import time\n",
                "import logging\n",
                "from botocore.exceptions import ClientError, WaiterError\n",
                "\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "\n",
                "def create_and_wait_for_endpoint(sagemaker_client, sess, endpoint_name, endpoint_config_name, max_wait_time=3600, check_interval=30):\n",
                "    try:\n",
                "        # Create the endpoint\n",
                "        ep_response = sagemaker_client.create_endpoint(\n",
                "            EndpointName=endpoint_name,\n",
                "            EndpointConfigName=endpoint_config_name,\n",
                "        )\n",
                "        logging.info(f\"Creating endpoint: {endpoint_name}\")\n",
                "        \n",
                "        # Wait for the endpoint to be created\n",
                "        start_time = time.time()\n",
                "        while True:\n",
                "            try:\n",
                "                sess.wait_for_endpoint(endpoint_name, poll=check_interval)\n",
                "                logging.info(f\"Endpoint {endpoint_name} is now in service\")\n",
                "                break\n",
                "            except WaiterError as e:\n",
                "                if \"Max attempts exceeded\" in str(e):\n",
                "                    current_time = time.time()\n",
                "                    if current_time - start_time > max_wait_time:\n",
                "                        logging.error(f\"Endpoint creation timed out after {max_wait_time} seconds\")\n",
                "                        raise TimeoutError(f\"Endpoint creation timed out\")\n",
                "                    else:\n",
                "                        logging.info(\"Endpoint is still being created. Continuing to wait...\")\n",
                "                else:\n",
                "                    raise\n",
                "        \n",
                "        return ep_response\n",
                "    \n",
                "    except ClientError as e:\n",
                "        logging.error(f\"Error creating endpoint: {e}\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Unexpected error: {e}\")\n",
                "        raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Usage\n",
                "try:\n",
                "    endpoint_name = f\"{registered_model}-endpoint-{currentTime}\"\n",
                "    logging.info(f\"Endpoint name: {endpoint_name}\")\n",
                "    \n",
                "    ep_response = create_and_wait_for_endpoint(sagemaker_client, sess, endpoint_name, endpoint_config_name)\n",
                "    logging.info(\"Endpoint created successfully\")\n",
                "except Exception as e:\n",
                "    logging.error(f\"Failed to create endpoint: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SageMaker Inference Component Creation and Inference Execution\n",
                "\n",
                "This section demonstrates how to create inference components for both pre-trained and fine-tuned models, then run comprehensive testing to compare their Chain-of-Thought reasoning capabilities."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Defining SageMaker Model\n",
                "\n",
                "First, let's verify that our compressed model artifacts are available in S3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!aws s3 ls $compressed_model_path/finetuned/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!aws s3 ls $compressed_model_path/pretrained/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Set and Model Configuration\n",
                "\n",
                "We'll set up comprehensive test prompts covering various domains to evaluate the Chain-of-Thought reasoning capabilities of both models. The test suite includes questions about tourism, culinary arts, philosophy, and AI behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from huggingface_hub import HfFolder\n",
                "from sagemaker.huggingface import HuggingFaceModel\n",
                "import logging\n",
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "# Create results storage directory\n",
                "results_dir = \"inference_results\"\n",
                "os.makedirs(results_dir, exist_ok=True)\n",
                "\n",
                "# Test prompts covering various domains and complexity levels\n",
                "user_prompts = [\n",
                "    'Please provide detailed information about famous tourist routes in Seoul',\n",
                "    'Please provide detailed information about famous tourist routes in Goseong, Gyeongsangnam-do',\n",
                "    'How many Michelin 3-star restaurants are there worldwide?',\n",
                "    'Tell me about famous chefs in Korea',\n",
                "    'Tell me about famous chefs in America',\n",
                "    'What is the extent of your knowledge cutoff date?',\n",
                "    'Can you respond to inappropriate or sexual jokes?',\n",
                "    'Please explain the Analects of Confucius',\n",
                "    'You are male. Nurses are generally female. Can we say that all nurses are female?'\n",
                "]\n",
                "\n",
                "# Chain-of-Thought inference prompt template\n",
                "inference_prompt_style = \"\"\"You are an AI Assistant with advanced knowledge in reasoning, analysis, and problem-solving.\n",
                "Provide the most appropriate answer to the <question>. Before presenting your <final> answer, develop a step-by-step thought process (chain of thoughts) to perform logical and accurate analysis of the <question>.\n",
                "\n",
                "<question>\n",
                "{}\n",
                "</question>\n",
                "### Guidelines:\n",
                "- Skip unnecessary greetings or preambles, and start directly with <response>\n",
                "- Do not repeat the question and answer\n",
                "- Write the step-by-step thought process in sufficient detail, but keep the final answer concise\n",
                "\n",
                "### Response Format:\n",
                "<think>\n",
                "    ### THINKING\n",
                "    [Provide detailed step-by-step reasoning process here. Analyze the problem, consider possible approaches, and use logical reasoning to reach a conclusion.]\n",
                "</think>\n",
                "<final>\n",
                "    ### FINAL-ANSWER\n",
                "    [Present the conclusion derived from THINKING as a concise and clear final answer.]\n",
                "</final>\n",
                "\n",
                "Answer below:\n",
                "<think>\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "# inference_prompt_style.format(question) + tokenizer.eos_token\n",
                "\n",
                "# Define model types for comparison\n",
                "model_types = ['pretrained', 'finetuned']\n",
                "model_results = {}\n",
                "\n",
                "# Create SageMaker Runtime client\n",
                "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
                "\n",
                "# Common configuration for both models\n",
                "common_config = {\n",
                "    \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
                "    \"MAX_INPUT_LENGTH\": \"2048\",\n",
                "    \"MAX_TOTAL_TOKENS\": \"4096\",\n",
                "    \"MAX_BATCH_PREFILL_TOKENS\": \"4096\",\n",
                "    \"SM_NUM_GPUS\": \"1\"  # Use 1 GPU\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SageMaker Inference Component Creation and Test Data Response Verification\n",
                "\n",
                "This comprehensive testing process will:\n",
                "1. Create inference components for both pre-trained and fine-tuned models\n",
                "2. Run all test prompts through both models\n",
                "3. Collect performance metrics and response quality data\n",
                "4. Save results for comparative analysis\n",
                "5. Automatically clean up resources after testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "import time\n",
                "import boto3\n",
                "import re\n",
                "import json\n",
                "import os\n",
                "from IPython.display import display, HTML\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# SageMaker client setup\n",
                "sagemaker_client = boto3.client('sagemaker')\n",
                "\n",
                "model_name_list = []\n",
                "# Process each model type sequentially\n",
                "for model_type in model_types:\n",
                "    print(f\"\\n\\n===== PROCESSING {model_type.upper()} MODEL =====\")\n",
                "    \n",
                "    # Create model\n",
                "    model_name = f\"{registered_model}-{time.strftime('%Y-%m-%d-%H-%M-%S')}-{model_type}\"\n",
                "    ic_name = f\"IC-{model_name}\"  # Initialize variable\n",
                "    \n",
                "    model_name_list.append(model_name) ## Use for deletion\n",
                "    \n",
                "    print(f\"Creating model: {model_name}\")\n",
                "    \n",
                "    try:\n",
                "        # Create HuggingFace model\n",
                "        llm_model = HuggingFaceModel(\n",
                "            role=role,\n",
                "            name=model_name,\n",
                "            model_data=f\"{compressed_model_path}/{model_type}/model.tar.gz\",\n",
                "            image_uri=llm_image,\n",
                "            env=common_config\n",
                "        )\n",
                "        llm_model.create()\n",
                "        print(f\"Model {model_name} created successfully\")\n",
                "\n",
                "        # Create Inference Component\n",
                "        print(f\"Creating inference component: {ic_name}\")\n",
                "        \n",
                "        # Attempt to delete existing Inference Component\n",
                "        try:\n",
                "            sagemaker_client.delete_inference_component(InferenceComponentName=ic_name)\n",
                "            print(f\"Deleted existing inference component: {ic_name}\")\n",
                "        except Exception as e:\n",
                "            if 'ResourceNotFoundException' in str(e):\n",
                "                print(f\"Inference component {ic_name} does not exist. Skipping deletion.\")\n",
                "            else:\n",
                "                print(f\"Error deleting inference component {ic_name}: {e}\")\n",
                "        \n",
                "        # Create new Inference Component\n",
                "        spec = {\n",
                "            \"ModelName\": model_name,\n",
                "            \"ComputeResourceRequirements\": {\n",
                "                \"NumberOfAcceleratorDevicesRequired\": 1,  # Use 1 GPU\n",
                "                \"NumberOfCpuCoresRequired\": 4,\n",
                "                \"MinMemoryRequiredInMb\": 8192,\n",
                "            },\n",
                "        }\n",
                "        \n",
                "        ic_response = sagemaker_client.create_inference_component(\n",
                "            InferenceComponentName=ic_name,\n",
                "            EndpointName=endpoint_name,\n",
                "            VariantName=variant_name,\n",
                "            Specification=spec,\n",
                "            RuntimeConfig={\"CopyCount\": 1}\n",
                "        )\n",
                "        print(f\"Created inference component: {ic_name}\")\n",
                "        \n",
                "        # Wait for Inference Component to become InService\n",
                "        print(f\"Waiting for {ic_name} to be in service...\")\n",
                "        max_attempts = 30\n",
                "        sleep_time = 20\n",
                "        \n",
                "        for attempt in range(max_attempts):\n",
                "            desc = sagemaker_client.describe_inference_component(\n",
                "                InferenceComponentName=ic_name\n",
                "            )\n",
                "            status = desc['InferenceComponentStatus']\n",
                "            print(f\"{ic_name}: {status}\")\n",
                "            \n",
                "            if status == 'InService':\n",
                "                print(f\"{ic_name} is now in service.\")\n",
                "                break\n",
                "            elif status == 'Failed':\n",
                "                print(f\"{ic_name} has failed.\")\n",
                "                break\n",
                "            \n",
                "            print(f\"Attempt {attempt+1}/{max_attempts}. Waiting for {sleep_time} seconds...\")\n",
                "            time.sleep(sleep_time)\n",
                "        \n",
                "        # Run model inference and save results\n",
                "        print(f\"\\nRunning inference with {model_type} model...\")\n",
                "        model_results = {\"times\": [], \"answers\": [], \"full_responses\": []}\n",
                "        \n",
                "        for prompt_index, user_prompt in enumerate(user_prompts):\n",
                "            print(f\"\\nTesting prompt {prompt_index + 1}: {user_prompt}\")\n",
                "        \n",
                "            # Create inference request\n",
                "            request_body = {\n",
                "                \"inputs\": inference_prompt_style.format(user_prompt),\n",
                "                \"parameters\": {\n",
                "                    \"max_new_tokens\": 2048,\n",
                "                    \"top_p\": 0.9,\n",
                "                    \"temperature\": 0.01,\n",
                "                    \"use_cache\": True,\n",
                "                    \"stop\": [\"<|im_end|>\"]\n",
                "                }\n",
                "            }\n",
                "\n",
                "            start_time = time.time()\n",
                "            try:\n",
                "                # Execute inference request\n",
                "                response = sagemaker_runtime.invoke_endpoint(\n",
                "                    EndpointName=endpoint_name,\n",
                "                    InferenceComponentName=ic_name,\n",
                "                    ContentType='application/json',\n",
                "                    Body=json.dumps(request_body)\n",
                "                )\n",
                "                \n",
                "                # Process response\n",
                "                result = response['Body'].read().decode('utf-8')\n",
                "                parsed_data = json.loads(result)\n",
                "                answer = parsed_data[0] if isinstance(parsed_data, list) else parsed_data\n",
                "                generated_text = answer['generated_text']\n",
                "                \n",
                "                # Extract text after \"Answer below:\"\n",
                "                answer_marker = \"Answer below:\"\n",
                "                answer_start = generated_text.find(answer_marker)\n",
                "                if answer_start >= 0:\n",
                "                    actual_response = generated_text[answer_start + len(answer_marker):].strip()\n",
                "                else:\n",
                "                    actual_response = generated_text.strip()\n",
                "                \n",
                "                # Extract thinking process and final answer from response\n",
                "                thinking_part = \"\"\n",
                "                final_part = \"\"\n",
                "                \n",
                "                # Try tag-based extraction\n",
                "                think_pattern = re.compile(r'### THINKING\\s*(.*?)(?=### FINAL-ANSWER|\\$)', re.DOTALL)\n",
                "                final_pattern = re.compile(r'### FINAL-ANSWER\\s*(.*?)\\$', re.DOTALL)\n",
                "                \n",
                "                think_match = think_pattern.search(actual_response)\n",
                "                final_match = final_pattern.search(actual_response)\n",
                "                \n",
                "                if think_match and final_match:\n",
                "                    thinking_part = think_match.group(1).strip()\n",
                "                    final_part = final_match.group(1).strip()\n",
                "                    extraction_method = \"keywords\"\n",
                "                else:\n",
                "                    # Use full text\n",
                "                    final_part = actual_response.strip()\n",
                "                    thinking_part = \"Thinking process not clearly distinguished.\"\n",
                "                    extraction_method = \"full_text\"\n",
                "                \n",
                "                elapsed_time = time.time() - start_time\n",
                "                \n",
                "                # Save results\n",
                "                result_entry = {\n",
                "                    \"prompt\": user_prompt,\n",
                "                    \"thinking_process\": thinking_part,\n",
                "                    \"final_answer\": final_part,\n",
                "                    \"elapsed_time\": round(elapsed_time, 3),\n",
                "                    \"extraction_method\": extraction_method\n",
                "                }\n",
                "                \n",
                "                model_results[\"times\"].append(elapsed_time)\n",
                "                model_results[\"answers\"].append(result_entry)\n",
                "                model_results[\"full_responses\"].append(actual_response)  # Store actual response only\n",
                "                \n",
                "                print(f\"Elapsed time: {round(elapsed_time, 3)} seconds\")\n",
                "                print(f\"Final answer:\\n{final_part}\")\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"Error processing request: {str(e)}\")\n",
                "                \n",
                "                model_results[\"times\"].append(None)\n",
                "                model_results[\"answers\"].append({\n",
                "                    \"prompt\": user_prompt,\n",
                "                    \"thinking_process\": f\"Error: {str(e)}\",\n",
                "                    \"final_answer\": f\"Error: {str(e)}\",\n",
                "                    \"elapsed_time\": None,\n",
                "                    \"extraction_method\": \"error\"\n",
                "                })\n",
                "                model_results[\"full_responses\"].append(f\"Error: {str(e)}\")\n",
                "        \n",
                "        # Save results to file\n",
                "        result_file = f\"{results_dir}/{model_type}_results.json\"\n",
                "        with open(result_file, \"w\", encoding='utf-8') as f:\n",
                "            json.dump(model_results, f, indent=2, ensure_ascii=False)\n",
                "        print(f\"\\nResults for {model_type} model saved to {result_file}\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing model {model_type}: {str(e)}\")\n",
                "    finally:\n",
                "        # Always delete Inference Component to clean up resources\n",
                "        if 'ic_name' in locals():  # Check if ic_name is defined\n",
                "            try:\n",
                "                sagemaker_client.delete_inference_component(InferenceComponentName=ic_name)\n",
                "                print(f\"Deleted inference component: {ic_name}\")\n",
                "            except Exception as e:\n",
                "                print(f\"Error deleting inference component {ic_name}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Comparison of Pre-trained and Fine-tuned Model Results\n",
                "\n",
                "This section provides a comprehensive comparison of both models' performance, highlighting the improvements achieved through Chain-of-Thought fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Result comparison and visualization\n",
                "print(\"\\n\\n===== COMPARING RESULTS =====\")\n",
                "\n",
                "# Read saved result files\n",
                "all_results = {}\n",
                "for model_type in model_types:\n",
                "    result_file = f\"{results_dir}/{model_type}_results.json\"\n",
                "    try:\n",
                "        with open(result_file, \"r\", encoding='utf-8') as f:\n",
                "            all_results[model_type] = json.load(f)\n",
                "        print(f\"Loaded results for {model_type} model from {result_file}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading results for {model_type} model: {e}\")\n",
                "\n",
                "# First create summary table for all questions\n",
                "display(HTML(\"<h2>Summary of Results for All Questions</h2>\"))\n",
                "\n",
                "# Create HTML table directly\n",
                "html_table = \"\"\"\n",
                "<table style=\"width:100%; border-collapse:collapse; table-layout:fixed;\">\n",
                "  <thead>\n",
                "    <tr style=\"background-color:#f2f2f2;\">\n",
                "      <th style=\"width:2%; text-align:center; padding:8px; border:1px solid #ddd;\">No.</th>\n",
                "      <th style=\"width:20%; text-align:left; padding:8px; border:1px solid #ddd;\">Question</th>\n",
                "      <th style=\"width:39%; text-align:left; padding:8px; border:1px solid #ddd;\">Pre-trained Model Answer</th>\n",
                "      <th style=\"width:39%; text-align:left; padding:8px; border:1px solid #ddd;\">Fine-tuned Model Answer</th>\n",
                "    </tr>\n",
                "  </thead>\n",
                "  <tbody>\n",
                "\"\"\"\n",
                "\n",
                "# Organize model answers for each question\n",
                "for prompt_index, prompt in enumerate(user_prompts):\n",
                "    pretrained_answer = \"\"\n",
                "    finetuned_answer = \"\"\n",
                "    \n",
                "    # Extract pre-trained model answer\n",
                "    if \"pretrained\" in all_results and prompt_index < len(all_results[\"pretrained\"][\"answers\"]):\n",
                "        answer = all_results[\"pretrained\"][\"answers\"][prompt_index]\n",
                "        final_answer = answer.get('final_answer', 'N/A')\n",
                "        \n",
                "        try:\n",
                "            # Remove tags (all HTML tag formats)\n",
                "            final_answer = re.sub(r'<[^>]+>', '', final_answer)\n",
                "            \n",
                "            # Add line breaks around \"THINKING\" keyword (using HTML tags)\n",
                "            final_answer = re.sub(r'THINKING', '<strong>THINKING</strong><br><br>', final_answer)\n",
                "            \n",
                "            # Add line breaks around \"FINAL-ANSWER\" keyword (using HTML tags)\n",
                "            final_answer = re.sub(r'FINAL-ANSWER', '<br><br><strong>FINAL-ANSWER</strong><br><br>', final_answer)\n",
                "\n",
                "            # Markdown processing\n",
                "            # 1. Remove markdown headers\n",
                "            final_answer = re.sub(r'#+\\s+', '', final_answer)\n",
                "            # 2. Remove markdown bold\n",
                "            final_answer = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', final_answer)\n",
                "            # 3. Remove markdown italic\n",
                "            final_answer = re.sub(r'\\*(.*?)\\*', r'\\1', final_answer)\n",
                "            # 4. Remove markdown links - keep only text from [text](link) format\n",
                "            final_answer = re.sub(r'$$(.*?)$$$(.*?)$', r'\\1', final_answer)\n",
                "            # 5. Remove markdown list markers\n",
                "            final_answer = re.sub(r'^\\s*[-*+]\\s+', '', final_answer, flags=re.MULTILINE)\n",
                "            # 6. Remove numbered list markers\n",
                "            final_answer = re.sub(r'^\\s*\\d+\\.\\s+', '', final_answer, flags=re.MULTILINE)\n",
                "            # 7. Remove markdown code blocks\n",
                "            final_answer = re.sub(r'```.*?```', '', final_answer, flags=re.DOTALL)\n",
                "            # 8. Remove inline code\n",
                "            final_answer = re.sub(r'`(.*?)`', r'\\1', final_answer)\n",
                "            \n",
                "            # Convert line breaks to spaces (except HTML tags)\n",
                "            final_answer = re.sub(r'(?!<br>)\\n', ' ', final_answer).strip()\n",
                "            \n",
                "            # Reduce consecutive spaces to single space\n",
                "            final_answer = re.sub(r'\\s+', ' ', final_answer)\n",
                "            \n",
                "            pretrained_answer = final_answer\n",
                "        except Exception as e:\n",
                "            print(f\"Error processing pre-trained model markdown: {e}\")\n",
                "            pretrained_answer = \"Error occurred during processing\"\n",
                "    else:\n",
                "        pretrained_answer = \"No result\"\n",
                "    \n",
                "    # Extract fine-tuned model answer\n",
                "    if \"finetuned\" in all_results and prompt_index < len(all_results[\"finetuned\"][\"answers\"]):\n",
                "        answer = all_results[\"finetuned\"][\"answers\"][prompt_index]\n",
                "        final_answer = answer.get('final_answer', 'N/A')\n",
                "        \n",
                "        try:\n",
                "            # Remove tags (all HTML tag formats)\n",
                "            final_answer = re.sub(r'<[^>]+>', '', final_answer)\n",
                "            \n",
                "            # Add line breaks around \"THINKING\" keyword (using HTML tags)\n",
                "            final_answer = re.sub(r'THINKING', '<strong>THINKING</strong><br><br>', final_answer)\n",
                "            \n",
                "            # Add line breaks around \"FINAL-ANSWER\" keyword (using HTML tags)\n",
                "            final_answer = re.sub(r'FINAL-ANSWER', '<br><br><strong>FINAL-ANSWER</strong><br><br>', final_answer)\n",
                "\n",
                "            # Markdown processing\n",
                "            # 1. Remove markdown headers\n",
                "            final_answer = re.sub(r'#+\\s+', '', final_answer)\n",
                "            # 2. Remove markdown bold\n",
                "            final_answer = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', final_answer)\n",
                "            # 3. Remove markdown italic\n",
                "            final_answer = re.sub(r'\\*(.*?)\\*', r'\\1', final_answer)\n",
                "            # 4. Remove markdown links - keep only text from [text](link) format\n",
                "            final_answer = re.sub(r'$$(.*?)$$$(.*?)$', r'\\1', final_answer)\n",
                "            # 5. Remove markdown list markers\n",
                "            final_answer = re.sub(r'^\\s*[-*+]\\s+', '', final_answer, flags=re.MULTILINE)\n",
                "            # 6. Remove numbered list markers\n",
                "            final_answer = re.sub(r'^\\s*\\d+\\.\\s+', '', final_answer, flags=re.MULTILINE)\n",
                "            # 7. Remove markdown code blocks\n",
                "            final_answer = re.sub(r'```.*?```', '', final_answer, flags=re.DOTALL)\n",
                "            # 8. Remove inline code\n",
                "            final_answer = re.sub(r'`(.*?)`', r'\\1', final_answer)\n",
                "            \n",
                "            # Convert line breaks to spaces (except HTML tags)\n",
                "            final_answer = re.sub(r'(?!<br>)\\n', ' ', final_answer).strip()\n",
                "            \n",
                "            # Reduce consecutive spaces to single space\n",
                "            final_answer = re.sub(r'\\s+', ' ', final_answer)\n",
                "            \n",
                "            finetuned_answer = final_answer\n",
                "        except Exception as e:\n",
                "            print(f\"Error processing fine-tuned model markdown: {e}\")\n",
                "            finetuned_answer = \"Error occurred during processing\"\n",
                "    else:\n",
                "        finetuned_answer = \"No result\"\n",
                "    \n",
                "    # Add table row\n",
                "    html_table += f\"\"\"\n",
                "    <tr>\n",
                "      <td style=\"text-align:center; padding:8px; border:1px solid #ddd; vertical-align:top;\">{prompt_index + 1}</td>\n",
                "      <td style=\"text-align:left; padding:8px; border:1px solid #ddd; vertical-align:top; word-wrap:break-word;\">{prompt[:50] + \"...\" if len(prompt) > 50 else prompt}</td>\n",
                "      <td style=\"text-align:left; padding:8px; border:1px solid #ddd; vertical-align:top; word-wrap:break-word;\">{pretrained_answer}</td>\n",
                "      <td style=\"text-align:left; padding:8px; border:1px solid #ddd; vertical-align:top; word-wrap:break-word;\">{finetuned_answer}</td>\n",
                "    </tr>\n",
                "    \"\"\"\n",
                "\n",
                "html_table += \"\"\"\n",
                "  </tbody>\n",
                "</table>\n",
                "\"\"\"\n",
                "\n",
                "display(HTML(html_table))\n",
                "\n",
                "# # Display detailed results for each question\n",
                "# display(HTML(\"<h2>Detailed Results by Question</h2>\"))\n",
                "\n",
                "# for prompt_index, prompt in enumerate(user_prompts):\n",
                "#     display(HTML(f\"<h3>Question {prompt_index + 1}</h3>\"))\n",
                "#     display(HTML(f\"<p><b>{prompt}</b></p>\"))\n",
                "    \n",
                "#     # Display full responses from each model\n",
                "#     for model_type in model_types:\n",
                "#         if model_type in all_results and prompt_index < len(all_results[model_type][\"full_responses\"]):\n",
                "#             full_response = all_results[model_type][\"full_responses\"][prompt_index]\n",
                "            \n",
                "#             try:\n",
                "#                 # Remove tags (all HTML tag formats)\n",
                "#                 clean_response = re.sub(r'<[^>]+>', '', full_response)\n",
                "                \n",
                "#                 # Convert \"THINKING\" keyword to emphasis\n",
                "#                 clean_response = re.sub(r'THINKING', 'Thinking Process:', clean_response)\n",
                "                \n",
                "#                 # Convert \"FINAL-ANSWER\" keyword to emphasis and add line breaks\n",
                "#                 clean_response = re.sub(r'FINAL-ANSWER', '\\n\\nFinal Answer:\\n', clean_response)\n",
                "                \n",
                "#                 # Convert markdown bold to HTML\n",
                "#                 clean_response = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', clean_response)\n",
                "                \n",
                "#                 # Convert markdown italic to HTML\n",
                "#                 clean_response = re.sub(r'\\*(.*?)\\*', r'<em>\\1</em>', clean_response)\n",
                "                \n",
                "#                 # Reduce consecutive empty lines to single line\n",
                "#                 clean_response = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', clean_response)\n",
                "                \n",
                "#                 # Remove empty lines before headers\n",
                "#                 clean_response = re.sub(r'\\n\\s*\\n(#+\\s+|Thinking Process:|Final Answer:)', r'\\n\\1', clean_response)\n",
                "#                 clean_response = re.sub(r'^(\\s*\\n)+(#+\\s+|Thinking Process:|Final Answer:)', r'\\2', clean_response)\n",
                "                \n",
                "#                 # Remove leading whitespace from each line\n",
                "#                 clean_lines = []\n",
                "#                 for line in clean_response.split('\\n'):\n",
                "#                     clean_lines.append(line.lstrip())\n",
                "#                 clean_response = '\\n'.join(clean_lines)\n",
                "                \n",
                "#             except Exception as e:\n",
                "#                 print(f\"Error processing full response: {e}\")\n",
                "#                 clean_response = full_response  # Use original text if error occurs\n",
                "                \n",
                "#                 # Remove leading whitespace even if error occurred\n",
                "#                 clean_lines = []\n",
                "#                 for line in clean_response.split('\\n'):\n",
                "#                     clean_lines.append(line.lstrip())\n",
                "#                 clean_response = '\\n'.join(clean_lines)\n",
                "            \n",
                "#             display(HTML(f\"<h4>{model_type} Model</h4>\"))\n",
                "            \n",
                "#             # Display full response\n",
                "#             display(HTML(f\"\"\"\n",
                "#             <div style=\"border: 1px solid #ddd; padding: 10px; margin-bottom: 20px;\">\n",
                "#                 <pre style=\"white-space: pre-wrap; margin: 0;\">{clean_response}</pre>\n",
                "#             </div>\n",
                "#             \"\"\"))\n",
                "    \n",
                "#     display(HTML(\"<hr style='margin: 30px 0;'>\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Resource Cleanup\n",
                "\n",
                "Proper resource management is crucial to avoid unnecessary costs. This section provides systematic cleanup of all deployed resources including inference components, models, and endpoints."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sagemaker.predictor import Predictor\n",
                "\n",
                "predictor = Predictor(\n",
                "    endpoint_name=endpoint_name,\n",
                "    sagemaker_session=sess,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "for model_name in model_name_list:\n",
                "    try:\n",
                "        inference_component_name = f\"IC-{model_name}\"\n",
                "        print(f\"Deleting inference components: [b magenta]{inference_component_name} \")\n",
                "        \n",
                "        # Delete inference component\n",
                "        sagemaker_client.delete_inference_component(\n",
                "            InferenceComponentName=inference_component_name\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(f\"{e}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "try:\n",
                "    # for model_name in model_name_list:\n",
                "    # print(f\"Deleting model: {model_name}\")\n",
                "    predictor.delete_model()\n",
                "except Exception as e:\n",
                "    print(f\"{e}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "\n",
                "try:\n",
                "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} \")\n",
                "    predictor.delete_endpoint()\n",
                "except Exception as e:\n",
                "    print(f\"{e}\")\n",
                "\n",
                "print(\"---\" * 10)\n",
                "print(\"Done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# endpoint_name='qwen3-4b-endpoint-2025-05-04-09-29-33'\n",
                "# ic_name = 'IC-qwen3-4b-2025-05-04-09-29-33-finetuned'\n",
                "\n",
                "# sagemaker_client.describe_inference_component(InferenceComponentName=ic_name)\n",
                "# # sagemaker_client.delete_inference_component(InferenceComponentName=ic_name)\n",
                "\n",
                "# # Create new Inference Component\n",
                "# spec = {\n",
                "#     \"ModelName\": model_name,\n",
                "#     \"ComputeResourceRequirements\": {\n",
                "#         \"NumberOfAcceleratorDevicesRequired\": 1,\n",
                "#         \"NumberOfCpuCoresRequired\": 4,\n",
                "#         \"MinMemoryRequiredInMb\": 8192,\n",
                "#     },\n",
                "# }\n",
                "\n",
                "# ic_response = sagemaker_client.create_inference_component(\n",
                "#     InferenceComponentName=ic_name,\n",
                "#     EndpointName=endpoint_name,\n",
                "#     VariantName=variant_name,\n",
                "#     Specification=spec,\n",
                "#     RuntimeConfig={\"CopyCount\": 1}\n",
                "# )\n",
                "\n",
                "# user_prompt=\"How many Michelin 3-star restaurants are there worldwide?\"\n",
                "# # Create inference request\n",
                "# request_body = {\n",
                "#     \"inputs\": inference_prompt_style.format(user_prompt),\n",
                "#     \"parameters\": {\n",
                "#         \"max_new_tokens\": 2048,\n",
                "#         \"top_p\": 0.9,\n",
                "#         \"temperature\": 0.01,\n",
                "#         \"use_cache\" : True,\n",
                "#         \"stop\": [\"<|im_end|>\"]\n",
                "#     }\n",
                "# }\n",
                "\n",
                "\n",
                "# # Execute inference request\n",
                "# response = sagemaker_runtime.invoke_endpoint(\n",
                "#     EndpointName=endpoint_name,\n",
                "#     InferenceComponentName=ic_name,\n",
                "#     ContentType='application/json',\n",
                "#     Body=json.dumps(request_body)\n",
                "# )\n",
                "\n",
                "# # Process response\n",
                "# result = response['Body'].read().decode('utf-8')\n",
                "# parsed_data = json.loads(result)\n",
                "# answer = parsed_data[0] if isinstance(parsed_data, list) else parsed_data\n",
                "\n",
                "\n",
                "# print(answer['generated_text'])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_pytorch_p310",
            "language": "python",
            "name": "conda_pytorch_p310"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        },
        "vscode": {
            "interpreter": {
                "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}